## Suggested readings from the course:

 1. For a closer look at the arithmetic behind convolution, and how it is affected by your choice of padding scheme, stride and other parameters, please refer to this illustrated guide:<br>
[V. Dumoulin and F. Visin, A guide to convolution arithmetic for deep learning.](https://arxiv.org/pdf/1603.07285v1.pdf)

2. Paper that introduced LeNet5: <br>
[Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)

3.  Lecture Notes for 1 lecture from UCF course: [Deep Learning - UCF CS](http://www.cs.ucf.edu/~gqi/CAP5610/CAP5610Lecture10.pdf) -

4. Contimuous Bag of Words Model: <br>
[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)<br>
	Additional material:
	- [word2vec Parameter Learning Explained](https://arxiv.org/abs/1411.2738): From the paper's abstract:<br>
	This  note  provides  detailed  derivations  and  explanations  of  the  parameter  update equations of the word2vec models, including the original continuous bag-of-word (CBOW)  and  skip-gram  (SG)  models,  as  well  as  advanced  optimization  techniques, including hierarchical softmax and negative sampling.
